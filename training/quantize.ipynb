{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8z5mvPcJtQzlaMg+9gXK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnises/llmog/blob/finetuning/training/quantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quantize the model and upload to huggingface\n",
        "# uses unsloth for the quantization, so needs a gpu.\n",
        "# it is probably much better to do the quantization some other way."
      ],
      "metadata": {
        "id": "BEQjJfTWBvut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO specify the exact versions of dependencies here. This was written around the end of May 2025, so if unsloth has breaking changes try reverting to the version that was current at that time\n",
        "import os\n",
        "is_colab = any('COLAB_' in k for k in os.environ.keys())\n",
        "if is_colab:\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "cnwoGHIvRh_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYCddsOnQ-ja"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "model_name = 'gemma-3-1b-llmog'\n",
        "quantized_model_name = 'gemma-3-1b-llmog-GGUF'\n",
        "HF_REPO = f\"jnises/{model_name}\"\n",
        "QUANTIZED_HF_REPO = f\"jnises/{quantized_model_name}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = HF_REPO,\n",
        ")"
      ],
      "metadata": {
        "id": "t_P5vqE1RJps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(quantized_model_name, tokenizer)"
      ],
      "metadata": {
        "id": "hUE1WrPB0Nxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "        # need to be just the model name here, a full path results in confusing error messages\n",
        "        quantized_model_name,\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ],
      "metadata": {
        "id": "ovoZNf9dUzmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "wn2xsILD3WiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_file(path_or_fileobj='gemma-3-1b-llmog-GGUF.Q8_0.gguf', path_in_repo='gemma-3-1b-llmog-GGUF.Q8_0.gguf', repo_id=QUANTIZED_HF_REPO, repo_type='model', token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "KpekVxxi3gFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}