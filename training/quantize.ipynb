{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNckxuZRAktRzFxTLWopEA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnises/llmog/blob/finetuning/quantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm"
      ],
      "metadata": {
        "id": "cnwoGHIvRh_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "ZEaOQ_1eRrAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYCddsOnQ-ja"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "model_name = 'gemma-3-1b-llmog'\n",
        "quantized_model_name = 'gemma-3-1b-llmog-GGUF'\n",
        "HF_REPO = f\"jnises/{model_name}\"\n",
        "QUANTIZED_HF_REPO = f\"jnises/{quantized_model_name}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "#import torch\n",
        "\n",
        "# fourbit_models = [\n",
        "#     # 4bit dynamic quants for superior accuracy and low memory use\n",
        "#     \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "#     \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "#     \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "#     \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "#     # Other popular models!\n",
        "#     \"unsloth/Llama-3.1-8B\",\n",
        "#     \"unsloth/Llama-3.2-3B\",\n",
        "#     \"unsloth/Llama-3.3-70B\",\n",
        "#     \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "#     \"unsloth/Phi-4\",\n",
        "# ] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    #model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    model_name = HF_REPO,\n",
        "    #max_seq_length = 2048, # Choose any for long context!\n",
        "    #max_seq_length = 4096, # try with larger sequences\n",
        "    #load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    #load_in_4bit = False, # not needed on l4?\n",
        "    #load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    #full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "t_P5vqE1RJps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(quantized_model_name, tokenizer)"
      ],
      "metadata": {
        "id": "hUE1WrPB0Nxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "        # need to be just the model name here, a full path results in confusing error messages\n",
        "        quantized_model_name,\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "        #quantization_type='q4_k_m',\n",
        "    )"
      ],
      "metadata": {
        "id": "ovoZNf9dUzmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub_gguf(\n",
        "#         quantized_model_name,\n",
        "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "#         repo_id = QUANTIZED_HF_REPO,\n",
        "#         token = HF_TOKEN,\n",
        "#     )"
      ],
      "metadata": {
        "id": "lzXtBkEjTgbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "fZmX2yWcTC2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "wn2xsILD3WiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_file(path_or_fileobj='gemma-3-1b-llmog-GGUF.Q8_0.gguf', path_in_repo='gemma-3-1b-llmog-GGUF.Q8_0.gguf', repo_id=QUANTIZED_HF_REPO, repo_type='model', token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "KpekVxxi3gFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxK0rgnOrcMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}