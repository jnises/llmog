{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from collections import deque\n",
    "import json\n",
    "import gzip\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.environ['OPENROUTER_API_KEY']\n",
    ")\n",
    "\n",
    "output = Path('output')\n",
    "conversations_root = Path('conversations')\n",
    "conversations_root.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb16d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for retry in range(10000):\n",
    "  start_time = time.time()\n",
    "  completion = client.chat.completions.create(\n",
    "    #model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    #model='google/gemini-2.5-pro-exp-03-25',\n",
    "    #model='deepseek/deepseek-r1:free',\n",
    "    #model='google/gemini-2.0-flash-exp:free',\n",
    "    #model='meta-llama/llama-4-maverick:free',\n",
    "    #model='deepseek/deepseek-chat:free',\n",
    "    #model='microsoft/mai-ds-r1:free',\n",
    "    #model='qwen/qwen3-235b-a22b:free',\n",
    "    #model='meta-llama/llama-4-scout:free',\n",
    "    #model='tngtech/deepseek-r1t-chimera:free',\n",
    "    #model='nvidia/llama-3.1-nemotron-ultra-253b-v1:free',\n",
    "    #model='google/gemma-3-27b-it:free',\n",
    "    #model='mistralai/mistral-small-3.1-24b-instruct:free',\n",
    "    model='google/gemini-2.0-flash-001',\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a plausible log file, as would be emitted from some application or service. It should contain both uninteresting and interesting lines. Reply ONLY with the log lines. No explanations, markdown quotes or any other form of framing.\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "  end_time = time.time()\n",
    "  content = completion.choices[0].message.content\n",
    "  if content.startswith('```'):\n",
    "      print('undesired framing in llm response. retrying')\n",
    "  else:\n",
    "      break\n",
    "\n",
    "print(content)\n",
    "output.mkdir(exist_ok=True, parents=True)\n",
    "for i in range(0, 1000000):\n",
    "    outpath = output / f'{i}.log'\n",
    "    if outpath.exists():\n",
    "        continue\n",
    "    break\n",
    "else:\n",
    "    assert False\n",
    "with outpath.open('w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "timings_path = Path('timings.txt')\n",
    "with timings_path.open('a') as f:\n",
    "    print(f'{outpath}\\t{completion.model}\\t{end_time - start_time}', file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_check = []\n",
    "for p in output.glob('*.log'):\n",
    "    logfile = p.read_text()\n",
    "    def fun(msg: str):\n",
    "      lmsg = msg.lower()\n",
    "      if 'yes' in lmsg:\n",
    "         return True\n",
    "      if 'no' in lmsg:\n",
    "         return False\n",
    "      raise Exception(f'bad reply from model: {msg}')\n",
    "    for retry in range(100000):\n",
    "      try:\n",
    "        completion = client.chat.completions.create(\n",
    "          model='meta-llama/llama-4-maverick:free',\n",
    "          messages=[\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"Does the following text look like a raw log file? It mustn't contain any additional framing, only the actual log lines. Answer ONLY with yes or no:\\n{logfile}\"\n",
    "            }\n",
    "          ]\n",
    "        )\n",
    "        if hasattr(completion, 'error') and completion.error:\n",
    "            raise Exception(f\"API call failed: {completion.error}\")\n",
    "        good = fun(completion.choices[0].message.content)\n",
    "        break\n",
    "      except Exception as e:\n",
    "        if retry < 10:\n",
    "          print(f'{e}. retrying..')\n",
    "        else:\n",
    "          raise\n",
    "    format_check.append((p, good))\n",
    "    print(f'{p}: {good}')\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a66a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "                \"role\": \"system\",\n",
    "                'content': '''You are a developer log analyzer.\n",
    "Given a sequence of log lines. Rate only the last line. Use the prior lines only for context.\n",
    "Rate that line by how interestingi you think that line is for diagnosing an issue with the system.\n",
    "Output EXACTLY in this format:\n",
    "```\n",
    "Very brief single-sentence analysis on a single line\n",
    "SCORE: 0-100\n",
    "```\n",
    "\n",
    "Do NOT include any code examples, snippets, or additional explanations.\n",
    "Keep responses strictly limited to the analysis and score.\n",
    "Do NOT include any additional framing such as markdown.\n",
    "\n",
    "Score guide:\n",
    "Low (0-30): Routine/minor info\n",
    "Medium (31-70): Noteworthy/important\n",
    "High (71-100): Critical/security issues\n",
    "'''\n",
    "}\n",
    "formatre = re.compile(r'^.*\\nSCORE: (?:100|\\d{1,2})$')\n",
    "\n",
    "#for log_path in [output / '0.log']:\n",
    "for log_path in output.glob('*.log'):\n",
    "  print(f'evaluating {log_path}\\n===========\\n\\n')\n",
    "  conversations = []\n",
    "  history = deque(maxlen=3)\n",
    "  with log_path.open('r') as f:\n",
    "      for line in f.readlines():\n",
    "        line = line.rstrip()\n",
    "        history.append(line)\n",
    "        lines = ''.join((f'{l}\\n' for l in history))\n",
    "        query = {\n",
    "                   'role': 'user',\n",
    "                   'content': lines,\n",
    "                }\n",
    "        for retry in range(4):\n",
    "          completion = client.chat.completions.create(\n",
    "                #model='meta-llama/llama-4-maverick:free',\n",
    "                #model='google/gemini-2.0-flash-exp:free',\n",
    "                #model='meta-llama/llama-4-scout',\n",
    "                model='google/gemini-2.0-flash-001',\n",
    "                messages=[\n",
    "                  system_prompt,\n",
    "                  query,\n",
    "                ]\n",
    "              )\n",
    "          if hasattr(completion, 'error') and completion.error:\n",
    "            raise Exception(f\"API call failed: {completion.error}\")\n",
    "          assert completion.choices[0].message.role == 'assistant'\n",
    "          message = completion.choices[0].message.content\n",
    "          if formatre.match(message) is None:\n",
    "             print(f'bad reply from model: {message}')\n",
    "             continue\n",
    "          break\n",
    "        else:\n",
    "           print('the model is obstinate, ignoring this line')\n",
    "           continue\n",
    "        print(f'{lines}{message}\\n')\n",
    "        # TODO: make sure the reply follows the requested structure\n",
    "        conversations.append([\n",
    "           system_prompt,\n",
    "           query,\n",
    "           message,\n",
    "        ])\n",
    "  conversation_path = conversations_root / log_path.relative_to(output).with_suffix('.json.gz')\n",
    "  with gzip.open(conversation_path, 'wb') as f:\n",
    "      f.write(json.dumps(conversations).encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79978a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
