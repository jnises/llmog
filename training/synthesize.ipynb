{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade requests datasets"
      ],
      "metadata": {
        "id": "16UIeSFGT4bt"
      },
      "id": "16UIeSFGT4bt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "is_colab = any('COLAB_' in k for k in os.environ.keys())\n",
        "\n",
        "if is_colab:\n",
        "  from google.colab import userdata\n",
        "  HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "  OPENROUTER_API_KEY = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "else:\n",
        "  !pip install dotenv\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "  OPENROUTER_API_KEY = os.environ[\"OPENROUTER_API_KEY\"]\n"
      ],
      "metadata": {
        "id": "IBzAKyndS30Y"
      },
      "id": "IBzAKyndS30Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8bcdcd2",
      "metadata": {
        "id": "e8bcdcd2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from collections import deque\n",
        "import json\n",
        "import re\n",
        "import datasets\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Optional, Any, Dict\n",
        "import random\n",
        "\n",
        "logfiles_path = Path(\"logfiles\")\n",
        "non_logfiles_path = Path(\"non_logfiles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b27066f",
      "metadata": {
        "id": "5b27066f"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import requests\n",
        "\n",
        "CHAT_MODEL = \"google/gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "def chat(messages: List[Dict[str, str]]) -> str:\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        },\n",
        "        data=json.dumps(\n",
        "            {\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"messages\": messages,\n",
        "            }\n",
        "        ),\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    message = response.json()[\"choices\"][0][\"message\"]\n",
        "    assert message[\"role\"] == \"assistant\"\n",
        "    reply = message[\"content\"]\n",
        "    return reply\n",
        "\n",
        "\n",
        "def find_next_unused_file(root: Path, suffix: str) -> Path:\n",
        "    \"\"\"of the form [number][suffix]\"\"\"\n",
        "    assert root.exists()\n",
        "    # TODO: binary search\n",
        "    for i in range(0, 1000000):\n",
        "        outpath = root / f\"{i}{suffix}\"\n",
        "        if not outpath.exists():\n",
        "            return outpath\n",
        "    assert False\n",
        "\n",
        "_start_markdown_re = re.compile(r\"^```\\w*\\n\")\n",
        "_end_markdown_re = re.compile(r\"```\\n*$\")\n",
        "def strip_markdown_framing(s: str) -> str:\n",
        "    start = _start_markdown_re.search(s)\n",
        "    end = _end_markdown_re.search(s)\n",
        "    if start and end:\n",
        "        return s[start.end() : end.start()]\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb16d13",
      "metadata": {
        "id": "cbb16d13"
      },
      "outputs": [],
      "source": [
        "# synthesize a bunch of log files. put them in `logfiles`\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Optional\n",
        "\n",
        "NUM_FILES_TO_GENERATE = 100\n",
        "\n",
        "# set max_workers to some high value to generate data faster\n",
        "with ThreadPoolExecutor() as executor:\n",
        "\n",
        "    def gen_log() -> Optional[str]:\n",
        "        for _ in range(4):\n",
        "            content = chat(\n",
        "                [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"\"\"Generate a plausible log file, as would be emitted from some application or service.\n",
        "It should contain both uninteresting and interesting lines, including interesting lines that aren't clearly marked as that.\n",
        "Include poorly formatted log lines.\n",
        "Pick a random timestamp format, or don't include one at all. But use the same format for the entire file.\n",
        "Reply ONLY with the log lines. No explanations, markdown quotes or any other form of framing.\n",
        "            \"\"\",\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            content = strip_markdown_framing(content)\n",
        "            if _start_markdown_re.match(content) or _end_markdown_re.match(content):\n",
        "                print(\"undesired framing in llm response. retrying\")\n",
        "            else:\n",
        "                return content\n",
        "        print(\"model is stubborn. giving up\")\n",
        "        return None\n",
        "\n",
        "    logs = executor.map(lambda _: gen_log(), range(NUM_FILES_TO_GENERATE))\n",
        "    for log in logs:\n",
        "        if log is None:\n",
        "            continue\n",
        "        logfiles_path.mkdir(exist_ok=True, parents=True)\n",
        "        outpath = find_next_unused_file(logfiles_path, \".log\")\n",
        "        with outpath.open(\"w\") as f:\n",
        "            f.write(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7cf4f0",
      "metadata": {
        "id": "fa7cf4f0"
      },
      "outputs": [],
      "source": [
        "# synthesize a bunch of non-log files. put them in `non_logfiles` but with .txt extension\n",
        "\n",
        "\n",
        "NUM_NON_LOG_FILES = 50\n",
        "\n",
        "# set max_workers to some high value to generate data faster\n",
        "with ThreadPoolExecutor(max_workers=50) as executor:\n",
        "\n",
        "    def gen_non_log() -> Optional[str]:\n",
        "        for _ in range(4):\n",
        "            content = chat(\n",
        "                [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"\"\"Generate random text content that does NOT resemble a log file.\n",
        "It could be a snippet of an article, code, configuration, documentation, or other non-log content.\n",
        "Reply ONLY with the text content. No explanations, markdown quotes or any other form of framing.\n",
        "Do NOT start or end with ```.\n",
        "            \"\"\",\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            # the model likes to add markdown code framing even though we tell it not to. so we strip that out\n",
        "            content = strip_markdown_framing(content)\n",
        "            return content\n",
        "        print(\"model is stubborn. giving up\")\n",
        "        return None\n",
        "\n",
        "    non_logs = executor.map(lambda _: gen_non_log(), range(NUM_NON_LOG_FILES))\n",
        "    for non_log in non_logs:\n",
        "        if non_log is None:\n",
        "            continue\n",
        "        non_logfiles_path.mkdir(exist_ok=True, parents=True)\n",
        "        outpath = find_next_unused_file(non_logfiles_path, \".txt\")\n",
        "        with outpath.open(\"w\") as f:\n",
        "            f.write(non_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c475d326",
      "metadata": {
        "id": "c475d326"
      },
      "outputs": [],
      "source": [
        "def iterate_logs():\n",
        "    for logpath in logfiles_path.glob(\"*.log\"):\n",
        "        with logpath.open(\"rt\") as f:\n",
        "            yield {\"logs\": f.read()}\n",
        "\n",
        "\n",
        "hf_raw_logs_dataset = datasets.Dataset.from_generator(iterate_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b9a44e",
      "metadata": {
        "id": "b3b9a44e"
      },
      "outputs": [],
      "source": [
        "hf_raw_logs_dataset.push_to_hub(\"jnises/llmog-raw-logs\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d65325",
      "metadata": {
        "id": "86d65325"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "\n",
        "def iterate_non_logs():\n",
        "    for path in non_logfiles_path.glob(\"*.txt\"):\n",
        "        with path.open(\"rt\") as f:\n",
        "            yield {\"non_log\": f.read()}\n",
        "\n",
        "\n",
        "hf_raw_non_logs_dataset = datasets.Dataset.from_generator(iterate_non_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340c6b79",
      "metadata": {
        "id": "340c6b79"
      },
      "outputs": [],
      "source": [
        "hf_raw_non_logs_dataset.push_to_hub(\"jnises/llmog-raw-non-logs\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c2e68f",
      "metadata": {
        "id": "09c2e68f"
      },
      "outputs": [],
      "source": [
        "# ask the model if the generated files look good.\n",
        "# it isn't very accurate or useful\n",
        "\n",
        "if False:\n",
        "    format_check = []\n",
        "    for p in logfiles_path.glob(\"*.log\"):\n",
        "        logfile = p.read_text()\n",
        "\n",
        "        def fun(msg: str):\n",
        "            lmsg = msg.lower()\n",
        "            if \"yes\" in lmsg:\n",
        "                return True\n",
        "            if \"no\" in lmsg:\n",
        "                return False\n",
        "            raise Exception(f\"bad reply from model: {msg}\")\n",
        "\n",
        "        for retry in range(100000):\n",
        "            try:\n",
        "                response = chat(\n",
        "                    [\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": f\"Does the following text look like a raw log file? It mustn't contain any additional framing, only the actual log lines. Answer ONLY with yes or no:\\n{logfile}\",\n",
        "                        }\n",
        "                    ]\n",
        "                )\n",
        "                good = fun(response)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if retry < 10:\n",
        "                    print(f\"{e}. retrying..\")\n",
        "                else:\n",
        "                    raise\n",
        "        format_check.append((p, good))\n",
        "        print(f\"{p}: {'good' if good else 'bad'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "68a66a2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629,
          "referenced_widgets": [
            "04f276806f784d2b8d910164dc288067",
            "5aacb3ee49f249c0b0c37ec487a443ee",
            "f2feef26a8c440899eb115e274c817cc",
            "8e9dc942b8774ab1ae114c752c9f3ee7",
            "dc66bb9c596448e2a44a3a126d21efb9",
            "6e02e41d18234dbc805ad1588c9c30a1",
            "b6501a510901467a9a52b94fb1462581",
            "608084834c834bcaac69054d8a383e57",
            "0e1c3b537f8c47c0a79842aa82686e2a",
            "44d2c206a94f4e1fa8187c693c97894d",
            "fcfe7b5e6d134c3f92f85275af653fce"
          ]
        },
        "id": "68a66a2f",
        "outputId": "85e3dcb6-c125-4db3-bf93-571503fb6179"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04f276806f784d2b8d910164dc288067"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "DatasetGenerationError",
          "evalue": "An error occurred while generating the dataset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1625\u001b[0m                     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m                     \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_examples_on_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 ]\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mtyped_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_try_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m                 \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mtrying_cast_to_python_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_1d_for_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m# use smaller integer precisions if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: cannot mix struct and non-struct, non-null values",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1634\u001b[0m                 \u001b[0mnum_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshard_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m                 \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhkey_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_examples_on_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m         \u001b[0;31m# If schema is known, infer features even if no examples were written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 ]\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mtyped_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_try_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m                 \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mtrying_cast_to_python_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_1d_for_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m# use smaller integer precisions if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: cannot mix struct and non-struct, non-null values",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-26fdf2013304>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mhf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_conversations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         ).read()\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/io/generator.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.builder.download_and_prepare(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                         \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_proc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                     self._download_and_prepare(\n\u001b[0m\u001b[1;32m    926\u001b[0m                         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                         \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m         super()._download_and_prepare(\n\u001b[0m\u001b[1;32m   1650\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 raise OSError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                 for job_id, done, content in self._prepare_split_single(\n\u001b[0m\u001b[1;32m   1488\u001b[0m                     \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_prepare_split_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                 ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSchemaInferenceError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDatasetGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An error occurred while generating the dataset\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_num_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_num_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
          ]
        }
      ],
      "source": [
        "# synthesize conversations where the model rates each line in the log files\n",
        "\n",
        "# separate system prompt for the trainer as we know it will only be used for actual log files\n",
        "trainer_system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"You are a developer log analyzer.\n",
        "Given a sequence of log lines. Rate the last line by how interesting you think it is for diagnosing an issue with the system.\n",
        "Rate only the last line. Use the prior lines only for context.\n",
        "If a prior line looks unrelated to the last one, disregard it.\n",
        "Output EXACTLY in this format:\n",
        "```\n",
        "Very brief single-sentence analysis on a single line\n",
        "SCORE: 0-100\n",
        "```\n",
        "\n",
        "Do NOT include any code examples, snippets, or additional explanations.\n",
        "Keep responses strictly limited to the analysis and score.\n",
        "Do NOT include any additional framing such as ```.\n",
        "Do NOT start the analysis with \"The last line\" or similar redundant information.\n",
        "\n",
        "Score guide:\n",
        "Low (0-30): Routine/minor info\n",
        "Medium (31-70): Noteworthy/important\n",
        "High (71-100): Critical/security issues\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"You are a developer log analyzer.\n",
        "Given a sequence of lines of text. Determine if it looks like a log file or not.\n",
        "If it looks like a log rate the last line by how interesting you think it is for diagnosing an issue with the system.\n",
        "Rate only the last line. Use the prior lines only for context.\n",
        "If a prior line looks unrelated to the last one, disregard it.\n",
        "Output EXACTLY in this format:\n",
        "```\n",
        "Very brief single-sentence analysis on a single line\n",
        "SCORE: 0-100\n",
        "```\n",
        "\n",
        "If it doesn't look like a log file just respond with:\n",
        "```\n",
        "Not a log\n",
        "SCORE: 0\n",
        "```\n",
        "\n",
        "Do NOT include any code examples, snippets, or additional explanations.\n",
        "Keep responses strictly limited to the analysis and score.\n",
        "Do NOT include any additional framing such as ```.\n",
        "Do NOT start the analysis with \"The last line\" or similar redundant information.\n",
        "\n",
        "Score guide:\n",
        "Low (0-30): Routine/minor info\n",
        "Medium (31-70): Noteworthy/important\n",
        "High (71-100): Critical/security issues\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "formatre = re.compile(r\"^(?:'''\\n)?(?P<response>.*\\nSCORE: (?:100|\\d{1,2}))(?:\\n```\\n?)?$\")\n",
        "\n",
        "log_dataset = datasets.load_dataset('jnises/llmog-raw-logs', token=HF_TOKEN, split='train')['logs']\n",
        "non_log_dataset = datasets.load_dataset('jnises/llmog-raw-non-logs', token=HF_TOKEN, split='train')['non_log']\n",
        "\n",
        "\n",
        "def iterate_line_windows(files):\n",
        "    history = deque(maxlen=10)\n",
        "    for file in files:\n",
        "        for line in file.splitlines():\n",
        "            line = line.rstrip()\n",
        "            history.append(line)\n",
        "            num_lines = random.randint(1, len(history))\n",
        "            selected_lines = list(history)[-num_lines:]\n",
        "            lines = \"\".join((f\"{l}\\n\" for l in selected_lines))\n",
        "            yield lines\n",
        "\n",
        "\n",
        "def generate_conversations():\n",
        "    # set max_workers to some high value to generate data faster\n",
        "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
        "\n",
        "        def f(lines) -> Optional[Dict[str, Any]]:\n",
        "            query = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": lines,\n",
        "            }\n",
        "            for _ in range(4):\n",
        "                try:\n",
        "                    reply = chat(\n",
        "                        [\n",
        "                            trainer_system_prompt,\n",
        "                            query,\n",
        "                        ]\n",
        "                    )\n",
        "                except (\n",
        "                    ConnectionResetError,\n",
        "                    requests.exceptions.RequestException,\n",
        "                ) as e:\n",
        "                    # TODO: exponential backoff\n",
        "                    print(f\"exception communicating with model: {e}\")\n",
        "                    continue\n",
        "                # TODO: handle and strip out any ``` framing as the model sometimes produces that even though we tell it not to\n",
        "                if m := formatre.match(reply):\n",
        "                    reply = m.groups('response')\n",
        "                    # if reply == 'Not a log\\nSCORE: 0\\n':\n",
        "                    #     # we know this is a log. so just tell the model to try again\n",
        "                    #     continue\n",
        "                else:\n",
        "                    print(f\"bad reply from model: {reply}\")\n",
        "                    continue\n",
        "                break\n",
        "            else:\n",
        "                print(\"the model is obstinate, ignoring this line\")\n",
        "                return None\n",
        "            assert isinstance(system_prompt[\"content\"], str)\n",
        "            assert isinstance(lines, str)\n",
        "            assert isinstance(reply, str)\n",
        "            return {\n",
        "                \"conversations\": [\n",
        "                    system_prompt,\n",
        "                    query,\n",
        "                    {\"role\": \"assistant\", \"content\": reply},\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        results_iterator = executor.map(f, iterate_line_windows(log_dataset))\n",
        "        for non_log in iterate_line_windows(non_log_dataset):\n",
        "            yield {\n",
        "                'conversations': [\n",
        "                    system_prompt,\n",
        "                    {'role': 'user', 'content': non_log},\n",
        "                    {'role': 'assistant', 'content': 'Not a log\\nSCORE: 0\\n'}\n",
        "                ]\n",
        "            }\n",
        "        for result in results_iterator:\n",
        "            if result is not None:\n",
        "                yield result\n",
        "\n",
        "\n",
        "hf_dataset = datasets.Dataset.from_generator(generate_conversations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset"
      ],
      "metadata": {
        "id": "mZUoUV1hhZz3"
      },
      "id": "mZUoUV1hhZz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def7161f",
      "metadata": {
        "id": "def7161f"
      },
      "outputs": [],
      "source": [
        "hf_dataset = hf_dataset.train_test_split(test_size=0.1, seed=7834761)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c779b1",
      "metadata": {
        "id": "97c779b1"
      },
      "outputs": [],
      "source": [
        "hf_dataset.push_to_hub(\"jnises/llmog-conversations\", token=HF_TOKEN)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04f276806f784d2b8d910164dc288067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5aacb3ee49f249c0b0c37ec487a443ee",
              "IPY_MODEL_f2feef26a8c440899eb115e274c817cc",
              "IPY_MODEL_8e9dc942b8774ab1ae114c752c9f3ee7"
            ],
            "layout": "IPY_MODEL_dc66bb9c596448e2a44a3a126d21efb9"
          }
        },
        "5aacb3ee49f249c0b0c37ec487a443ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e02e41d18234dbc805ad1588c9c30a1",
            "placeholder": "​",
            "style": "IPY_MODEL_b6501a510901467a9a52b94fb1462581",
            "value": "Generating train split: "
          }
        },
        "f2feef26a8c440899eb115e274c817cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_608084834c834bcaac69054d8a383e57",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e1c3b537f8c47c0a79842aa82686e2a",
            "value": 1
          }
        },
        "8e9dc942b8774ab1ae114c752c9f3ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d2c206a94f4e1fa8187c693c97894d",
            "placeholder": "​",
            "style": "IPY_MODEL_fcfe7b5e6d134c3f92f85275af653fce",
            "value": " 999/0 [00:01&lt;00:00,  1.57s/ examples]"
          }
        },
        "dc66bb9c596448e2a44a3a126d21efb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e02e41d18234dbc805ad1588c9c30a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6501a510901467a9a52b94fb1462581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "608084834c834bcaac69054d8a383e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0e1c3b537f8c47c0a79842aa82686e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44d2c206a94f4e1fa8187c693c97894d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcfe7b5e6d134c3f92f85275af653fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}