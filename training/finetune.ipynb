{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnises/llmog/blob/finetuning/training/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PaE3ODY9Apy"
      },
      "source": [
        "This is based on the [unsloth](https://unsloth.ai/) gemma 3 finetune notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "is_colab = any('COLAB_' in k for k in os.environ.keys())\n",
        "\n",
        "if is_colab:\n",
        "  from google.colab import userdata\n",
        "  HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "else:\n",
        "  !pip install dotenv\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  HF_TOKEN = os.environ[\"HF_TOKEN\"]"
      ],
      "metadata": {
        "id": "6pHJDbz17TOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kcxp_u49Ap0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# TODO specify the exact versions of dependencies here. This was written around the end of May 2025, so if unsloth has breaking changes try reverting to the version that was current at that time\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbb0cuLzwgf"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length=2048,  # Choose any for long context!\n",
        "    load_in_4bit=False,  # we are training a 1B model, so we don't need to quantize much\n",
        "    load_in_8bit=True,\n",
        "    full_finetuning=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "# We now add LoRA adapters so we only need to update a small amount of parameters\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers=False,  # Turn off for just text!\n",
        "    finetune_language_layers=True,  # Should leave on!\n",
        "    finetune_attention_modules=True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules=True,  # SHould leave on always!\n",
        "    r=8,  # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha=8,  # Recommended alpha == r at least\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    random_state=3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6GHzFadqNWz"
      },
      "outputs": [],
      "source": [
        "# load the dataset from huggingface\n",
        "\n",
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"jnises/llmog-conversations\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "# We now use standardize_data_formats to try converting datasets to the correct format for finetuning purposes\n",
        "\n",
        "from unsloth.chat_templates import standardize_data_formats\n",
        "\n",
        "dataset = standardize_data_formats(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahE8Ys37JDJ"
      },
      "outputs": [],
      "source": [
        "# We now have to apply the chat template for Gemma-3 onto the conversations, and save it to text.\n",
        "# We remove the <bos> token using removeprefix('<bos>') since we're finetuning.\n",
        "# The Processor will add this token before training and the model expects only one.\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, tokenize=False, add_generation_prompt=False\n",
        "        ).removeprefix(\"<bos>\")\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=None,  # Can set up evaluation!\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,  # Use GA to mimic batch size!\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1,  # Set this for 1 full training run.\n",
        "        # max_steps = 30, # uncomment this and comment the num_train_epochs line to do a quicker run\n",
        "        learning_rate=2e-4,  # Reduce to 2e-5 for long training runs\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\",  # Use this for WandB etc\n",
        "        dataset_num_proc=2,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "# We also use Unsloth's train_on_completions method to only train on the assistant outputs and ignore the loss on the user's inputs.\n",
        "# This helps increase accuracy of finetunes.\n",
        "\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<start_of_turn>user\\n\",\n",
        "    response_part=\"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "# Let's train the model.\n",
        "# To resume a training run, set trainer.train(resume_from_checkpoint = True)\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# Test the trained model.\n",
        "# According to the Gemma-3 team, the recommended settings for inference are temperature = 1.0, top_p = 0.95, top_k = 64\n",
        "\n",
        "if False:\n",
        "    from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"gemma-3\",\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"\"\"You are a developer log analyzer.\n",
        "  Given a sequence of log lines. Rate only the last line. Use the prior lines only for context.\n",
        "  If a prior line looks unrelated to the last one, disregard it.\n",
        "  Rate the last line by how interesting you think it is for diagnosing an issue with the system.\n",
        "  Output EXACTLY in this format:\n",
        "  ```\n",
        "  Very brief single-sentence analysis on a single line\n",
        "  SCORE: 0-100\n",
        "  ```\n",
        "\n",
        "  Do NOT include any code examples, snippets, or additional explanations.\n",
        "  Keep responses strictly limited to the analysis and score.\n",
        "  Do NOT include any additional framing such as ````.\n",
        "  Do NOT start the analysis with \"The last line\" or similar redundant information.\n",
        "\n",
        "  Score guide:\n",
        "  Low (0-30): Routine/minor info\n",
        "  Medium (31-70): Noteworthy/important\n",
        "  High (71-100): Critical/security issues\n",
        "  \"\"\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"\"\"[2025-02-25 08:07:33] [INFO] Transaction started - xid=a7f392e1\n",
        "  [2025-02-25 08:07:33] [WARNING] Slow database query detected - duration=1530ms - query=\"SELECT * FROM orders JOIN order_items WHERE customer_id = ?\"\n",
        "  [2025-02-25 08:07:33] [INFO] Transaction committed - xid=a7f392e1\n",
        "  \"\"\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,  # Must add for generation\n",
        "        # this seems to be needed for the 1B model\n",
        "        tokenize=False,\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "        **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "        max_new_tokens=64,  # Increase for longer outputs!\n",
        "        # Recommended Gemma-3 settings!\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        top_k=64,\n",
        "    )\n",
        "    tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-CiKPrIFG0"
      },
      "outputs": [],
      "source": [
        "# upload the unquantized model to huggingface\n",
        "\n",
        "model_name = \"gemma-3-1b-llmog\"\n",
        "HF_REPO = f\"jnises/{model_name}\"\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    HF_REPO,\n",
        "    tokenizer,\n",
        "    token=HF_TOKEN,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}